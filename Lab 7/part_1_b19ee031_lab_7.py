# -*- coding: utf-8 -*-
"""Part-1 B19EE031 Lab_7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/159wwNboQK18HL0GV9cviLYQlQa_fi5lc
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas  as pd 
import numpy as np 
import matplotlib.pyplot as plt 
import seaborn as sns 
import sklearn
# %matplotlib inline

"""Creating a model with 3 inputs and 2 target variables. We have a hidden layer with 2 nodes. Both layers, the input layer and the hidden layer have a bias node.

Initializing weights and biases
"""

epochs = 10000

#Initializing the weights
w1,w2,w3,w4,w5,w6,w7,w8,w9,w10 = 0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.1
wlist = [w1,w2,w3,w4,w5,w6,w7,w8,w9,w10]
print(wlist)

#Intializing biases
b1,b2 = 0.5,0.5
blist = [b1,b2]
print(blist)

#Input variables
x1,x2,x3 = 1,4,5
xlist = [x1,x2,x3]
print(xlist)

#Target values
t1,t2 = 0.1, 0.05
tlist = [t1,t2]
print(tlist)

"""Defining learning rate, activation function and error. 
We are using sigmoid activation and mean squared error as error function
"""

alpha = 0.01

def sigmoid(x):
  return(np.divide(1,(1+np.exp(-x))))

def error(original_list, tlist):
  return(0.5* (np.power(original_list[0]-tlist[0],2) + np.power(original_list[1]-tlist[1],2)))

"""Defining Forward Propagation"""

# we assign alternate weights to input variables for proper training
def forwardProp(xlist, wlist, blist):
  zh1 = wlist[0]*xlist[0] + wlist[2]*xlist[1] + wlist[4]*xlist[2]
  zh2 = wlist[1]*xlist[0] + wlist[3]*xlist[1] + wlist[5]*xlist[2]

  h1 = sigmoid(zh1)
  h2 = sigmoid(zh2)

  z1 = wlist[6]*h1 + wlist[8]*h2 + blist[0]
  z2 = wlist[7]*h1 + wlist[9]*h2 + blist[1]

  o1 = sigmoid(z1)
  o2 = sigmoid(z2)

  return(h1,h2,o1,o2)

"""Backpropagation and training"""

errlist = []
for i in range(epochs):
  h1,h2,o1,o2 = forwardProp(xlist,wlist,blist)
  sse = error([o1,o2],tlist)
  errlist.append(sse)

  print('epoch no. = {} \n output1 = {}, output2 = {}, error = {}'.format(i+1,o1,o2,sse))

  #computing d(error)/dw7
  dE_do1 = o1 - t1
  do1_dz1 = o1*(1-o1)
  dz1_dw7 = h1
  dE_dw7 = dE_do1 * do1_dz1 * dz1_dw7

  #computing d(error)/dw8
  dE_do2 = o2 - t2
  do2_dz2 = o2*(1-o2)
  dz2_dw8 = h2
  dE_dw8 = dE_do2 * do2_dz2 * dz2_dw8

  # computing d(error)/dw9 and d(error)/dw10
  dz1_dw9 = h2
  dE_dw9 = dE_do1 * do1_dz1 * dz1_dw9
  dz2_dw10 = h2
  dE_dw10 = dE_do2 * do2_dz2 * dz2_dw10


  # Compute d(Error)/db2
  dz1_db2 = 1
  dz2_db2 = 1
  dE_db2 = dE_do1* do1_dz1* dz1_db2+ dE_do2* do2_dz2* dz2_db2 
  
  # Compute d(Error)/dh1 
  dz1_dh1= w7
  dz2_dh1 = w8
  dE_dh1 = dE_do1* do1_dz1* dz1_dh1+ dE_do2* do2_dz2* dz2_dh1

  # Compute d(Error)/dw1 
  dh1_dzh1 = h1*(1-h1)
  dzh1_dw1 = x1
  dE_dw1 =  dE_dh1* dh1_dzh1* dzh1_dw1

  # Compute d(Error)/dw3
  dzh1_dw3 = x2
  dE_dw3= dE_dh1* dh1_dzh1* dzh1_dw3

  # Compute d(Error)/dws
  dzh1_dw5 = x3
  dE_dw5= dE_dh1* dh1_dzh1* dzh1_dw5 
  
  # Compute d(Error)/dh2 first
  dz1_dh2 = w9
  dz2_dh2 = w10
  dE_dh2 = dE_do1* do1_dz1* dz1_dh2 + dE_do2* do2_dz2 * dz2_dh2

  # Compute d(Error)/dw2
  dh2_dzh2 =  h2*(1 - h2)
  dzh2_dw2 = x1
  dE_dw2 = dE_dh2* dh2_dzh2 * dzh2_dw2 

  # Compute d(Error)/dw4
  dzh2_dw4=x2
  dE_dw4 = dE_dh2* dh2_dzh2* dzh2_dw4

  # Compute d(Error)/dw6
  dzh2_dw6 = x3
  dE_dw6 = dE_dh2* dh2_dzh2* dzh2_dw6

  # Compute d(Error)/db1
  dzh1_db1 = 1
  dzh2_db1 = 1
  term1 = dE_do1* do1_dz1* dz1_dh1* dh1_dzh1* dzh1_db1
  term2 = dE_do2* do2_dz2* dz2_dh2* dh2_dzh2* dzh2_db1
  dE_db1 = term1 + term2
  
  #Updating all parameters
  w1 = w1 - alpha * dE_dw1
  w2 = w2 - alpha * dE_dw2
  w3 = w3 - alpha * dE_dw3
  w4 = w4 - alpha * dE_dw4
  w5 = w5 - alpha * dE_dw5
  w6 = w6 - alpha * dE_dw6
  w7 = w7 - alpha * dE_dw7
  w8 = w8 - alpha * dE_dw8
  w9 = w9 - alpha * dE_dw9
  w10 = w10 - alpha * dE_dw10
  b1 = b1 - alpha * dE_db1
  b2 = b2 - alpha * dE_db2
  wlist = [w1,w2,w3,w4,w5,w6,w7,w8,w9,w10]
  blist = [b1,b2]

pd.DataFrame(errlist, columns = ['SSE']).plot()

"""Building the same model using keras"""

from keras.models import Sequential
from keras.layers import Dense

model = Sequential()
model.add(Dense(3, input_dim =3, activation = 'relu'))
model.add(Dense(2))
model.compile(loss = 'mse', optimizer = 'adam', metrics = ['accuracy'])
his = model.fit([xlist],[tlist],epochs = 1000)

print(his.history.keys())

loss_values = his.history['loss']
epochs = range(1,len(loss_values)+1)
plt.plot(epochs, loss_values, 'b', label = 'Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Variation of loss with each iteration')
plt.plot