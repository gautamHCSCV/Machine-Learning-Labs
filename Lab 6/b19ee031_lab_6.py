# -*- coding: utf-8 -*-
"""B19EE031 Lab_6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ig42c0JWjz2CBqLbI-R5pBr0rrn4ok6A
"""

import pandas  as pd #Data manipulation
import numpy as np #Data manipulation
import matplotlib.pyplot as plt # Visualization
import seaborn as sns #Visualization
plt.rcParams['figure.figsize'] = [8,5]
plt.rcParams['font.size'] =14
plt.rcParams['font.weight']= 'bold'
plt.style.use('seaborn-whitegrid')

df = pd.read_csv('/content/insurance.csv')
print('\nNumber of rows and columns in the data set: ',df.shape)

df.head()

# desribe the dataset (Exploratory data analysis) 
df.describe()

x = range(df.shape[0])
for i in df.columns:
  plt.scatter(x,df[i])
  plt.title(i)
  plt.show()

#Check for missing value
df.isna().sum()

# correlation plot
df.corr()

corr = df.corr()
corr.style.background_gradient(cmap='coolwarm')

#Plot the distribution of the dependent variable
f= plt.figure(figsize=(15,5))
ax=f.add_subplot(121)
i = list(range(df.shape[0]))
ax.plot(i,df.loc[i,'charges'])
ax.set_title('Distribution of insurance charges')

ax=f.add_subplot(122)
ax.plot(i,np.log(df.loc[i,'charges']))
ax.set_title('Distribution of insurance charges in $log$ sacle')

"""#Convert categorical data into numbers


*   Label Enocding
*   One hot Encoding


#Lable Encoding
Label encoding refers to transforming the word labels into numerical form so that the algorithms can understand how to operate on them.

#One hot Encoding
A One hot encoding is a representation of categorical variable as binary vectors.It allows the representation of categorical data to be 
more expresive. This first requires that the categorical values be mapped to integer values, that is label encoding. Then, each integer 
value is represented as a binary vector that is all zero values except the index of the integer, which is marked with a 1.

You may take help of pandas get_dummies function for this. 
"""

from sklearn.preprocessing import LabelEncoder
enc = LabelEncoder()
df['sex'] = enc.fit_transform(df['sex'])
df['smoker'] = enc.fit_transform(df['smoker'])
df['region'] = enc.fit_transform(df['region'])
df.head()

# Log transform of dependent variable
df['log_transform'] = np.log(df['charges'])
df.head()

#Train Test split
from sklearn.model_selection import train_test_split
x = df.loc[:,'age':'region']
y = df['log_transform']
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3)
x_train.head()

"""#Model building"""

# Step 1: add x0 =1 to dataset
import copy
x_train_new = copy.deepcopy(x_train)
x_test_new = copy.deepcopy(x_test)
x_train_new['xo'] = 1
x_test_new['xo'] = 1
x_train_new.head()

# Step2: build model Î¸= inverse(X_transpose * X)*X_transpose*y 
def theta(x,y):
  x = np.array(x)
  a = np.dot(x.transpose(),x)
  #print(x,a)
  return(np.dot(np.dot(np.linalg.inv(a),x.transpose()),y))
parameters = theta(x_train_new,y_train)

# The parameters for linear regression model
print(parameters)

# Scikit Learn module
from sklearn.linear_model import LinearRegression
model_sk = LinearRegression()
model_sk.fit(x_train,y_train)

#Parameter
params = model_sk.get_params()
print('weights : ', model_sk.coef_)
print('Intercept : ', model_sk.intercept_)
params

"""#Model evaluation"""

# prediction
y_pred =  []
n = x_test_new.shape[0]
for i in range(n):
  ans = 0
  for j in range(len(x_test_new.columns)):
    ans+=parameters[j]*x_test_new.iloc[i,j]
  y_pred.append(ans)
y_pred

#Evaluvation: MSE (Write your MSE equation from scratch)
def mse(y_test,y_pred):
  sum = 0
  n = y_test.shape[0]
  for i in range(n):
    sum += (y_test.iloc[i]-y_pred[i])**2
  return sum/n
J_mse = mse(y_test,y_pred)
J_mse

print('The Mean Square Error(MSE) or J(theta) is: ',J_mse)

# sklearn regression module
y_pred_sk = model_sk.predict(x_test)
y_pred_sk

#Evaluvation: MSE
from sklearn.metrics import mean_squared_error
J_mse_sk = mean_squared_error(y_test,y_pred_sk)

print('The Mean Square Error(MSE) or J(theta) is: ',J_mse_sk)

print(x_test.shape,y_test.shape)

# Check for Linearity
import seaborn as sns
f = plt.figure(figsize=(15,5))
ax = f.add_subplot(121)
#relationship between independent and dependent variables
for i in df.columns[:-2]:
  sns.regplot(df[i],df.log_transform,data = df)
  plt.show()

#comparison between actual and predicted values
f = plt.figure(figsize=(20,10))
plt.scatter(list(range(n)),y_test,label = 'actual', color = 'b')
plt.scatter(list(range(n)),y_pred_sk, label = 'predicted by sklearn model', color = 'g')
plt.legend()
plt.show()

f = plt.figure(figsize=(20,10))
plt.scatter(list(range(n)),y_test,label = 'actual', color = 'b')
plt.scatter(list(range(n)),y_pred, label = 'predicted by built model', color = 'g')
plt.legend()
plt.show()