# -*- coding: utf-8 -*-
"""B19EE031 Lab_Exercise-5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/179UxMwifM2A7gj42Qq4K_GtkRdBGQvyr

Dependencies: Add Your dependencies here
"""

import pandas as pd
import numpy as np
import sklearn
import matplotlib.pyplot as plt
import nltk
import seaborn as sns

"""Load the data"""

df = pd.read_csv('train.csv')
df.head()

df.shape

"""Plot the count for each target"""

from collections import Counter
c = Counter(df['target'])
print(c)

plt.hist(df['target'])
plt.show()

"""Print the unique keywords"""

unique_keywords = df['keyword'].unique()
unique_keywords = unique_keywords[1:]
unique_keywords

"""Plot the count of each keyword"""

c = Counter(df['keyword'])
counts = []
for i in unique_keywords:
  counts.append(c[i])
print(c)

fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
ax.bar(unique_keywords,counts)
plt.show()

#Representaion of count of 4 words
fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
ax.bar(unique_keywords[:4],counts[:4])
plt.show()

"""Is there any correlation of the length of a tweet with its target. Try to visualize"""

for i in range(df.shape[0]): 
  df.loc[i,'len_tweet'] = len(df.iloc[i,3])
df.head()

relation = df[['len_tweet','target']].corr()
relation

#There is a positive correlation
plt.scatter(df['len_tweet'],df['target'])
plt.xlabel('Length of tweet')
plt.ylabel('target')
plt.show()

"""Print the number of null values in each column"""

for i in df.columns:
  print(i+ ' column has {} NULL values'.format(df[i].isna().sum()))

"""Remove the null values"""

df = df.dropna(axis = 'rows')
df.head()

"""Remove:


1.   Double Spaces
2.   Hypens and arrows
3.   Emojis
4.   URL
5.   Any other non english or special symbol

Replace wrong spellings with correct spellings


"""

import string
import re
def remove_urls(text):
    url_pattern = re.compile(r'https?://\S+|www\.\S+')
    return url_pattern.sub(r'', text)

def double_spaces(text):
  text = text.replace('  ',' ')
  return text
def remove_emoji(string):
    emoji_pattern = re.compile("["
                           u"\U0001F600-\U0001F64F"  # emoticons
                           u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                           u"\U0001F680-\U0001F6FF"  # transport & map symbols
                           u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                           u"\U00002702-\U000027B0"
                           u"\U000024C2-\U0001F251"
                           "]+", flags=re.UNICODE)
    return emoji_pattern.sub(r'', string)
def remove_punctuation(text):
    return text.translate(str.maketrans('', '', string.punctuation))

!pip install pyspellchecker
from spellchecker import SpellChecker

spell = SpellChecker()
def correct_spellings(text):
    corrected_text = []
    misspelled_words = spell.unknown(text.split())
    for word in text.split():
        if word in misspelled_words:
            corrected_text.append(spell.correction(word))
        else:
            corrected_text.append(word)
    return " ".join(corrected_text)

for i in range(df.shape[0]):
  df.iloc[i,3] = remove_urls(df.iloc[i,3])
  df.iloc[i,3] = double_spaces(df.iloc[i,3])
  df.iloc[i,3] = remove_punctuation(df.iloc[i,3])
  df.iloc[i,3] = remove_emoji(df.iloc[i,3])
  df.iloc[i,3] = correct_spellings(df.iloc[i,3])

df.head()

"""Plot a word cloud of real target and fake target"""

!pip install wordcloud
from wordcloud import WordCloud, STOPWORDS
comment_words = ''
stopwords = set(STOPWORDS)
for val in df.text:
      
    # typecaste each val to string
    val = str(val)
  
    # split the value
    tokens = val.split()
      
    # Converts each token into lowercase
    for i in range(len(tokens)):
        tokens[i] = tokens[i].lower()
      
    comment_words += " ".join(tokens)+" "
  
wordcloud = WordCloud(width = 800, height = 800,
                background_color ='white',
                stopwords = stopwords,
                min_font_size = 10).generate(comment_words)
  
# plot the WordCloud image                       
plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)
  
plt.show()

x1 = [] # text for target = 1
x0 = [] # text for target = 0
for i in range(df.shape[0]):
  if df.iloc[i,3]==1:
    x1.append(df.iloc[i,0])
  else:
    x0.append(df.iloc[i,0])

# for real target
for val in x1:
      
    # typecaste each val to string
    val = str(val)
  
    # split the value
    tokens = val.split()
      
    # Converts each token into lowercase
    for i in range(len(tokens)):
        tokens[i] = tokens[i].lower()
      
    comment_words += " ".join(tokens)+" "
  
wordcloud = WordCloud(width = 800, height = 800,
                background_color ='white',
                stopwords = stopwords,
                min_font_size = 10).generate(comment_words)
  
# plot the WordCloud image                       
plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)
  
plt.show()

#for fake target
for val in x0:
      
    # typecaste each val to string
    val = str(val)
  
    # split the value
    tokens = val.split()
      
    # Converts each token into lowercase
    for i in range(len(tokens)):
        tokens[i] = tokens[i].lower()
      
    comment_words += " ".join(tokens)+" "
  
wordcloud = WordCloud(width = 800, height = 800,
                background_color ='white',
                stopwords = stopwords,
                min_font_size = 10).generate(comment_words)
  
# plot the WordCloud image                       
plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)
  
plt.show()

"""Keep only text and target column in the dataset"""

df = df.iloc[:,3:5]
df.head()

"""Split data into train and validation"""

from sklearn.model_selection import train_test_split
x = df['text']
y = df['target']
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2)
x_test.head()

"""Print the count of unique words"""

words = []
for i in range(df.shape[0]):
  words+=df.iloc[i,0].split()
c = Counter(words)
print(c)

"""Compute the Term-Document Matrix (TDM) for all classes.

Use CountVectorizer of sklearn and print the dataframe with number of columns = number of unique words and the row showing the count of that word in a sentence.
"""

from sklearn.feature_extraction.text import CountVectorizer

vec = CountVectorizer()
X = vec.fit_transform(x)
df_new = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())
df_new

x1 = [] # text for target = 1
x0 = [] # text for target = 0
for i in range(df.shape[0]):
  if df.iloc[i,1]==1:
    x1.append(df.iloc[i,0])
  else:
    x0.append(df.iloc[i,0])

# class of target = 1
X1 = vec.fit_transform(x1)
df_new1 = pd.DataFrame(X1.toarray(), columns=vec.get_feature_names())
df_new1

# class of target = 0
X0 = vec.fit_transform(x0)
df_new0 = pd.DataFrame(X0.toarray(), columns=vec.get_feature_names())
df_new0

"""Frequency of words in class 0 and 1"""

words0 = []
for i in range(len(x0)):
  words0 += x0[i].split(' ')
c0 = Counter(words0)
print(c0)

words1 = []
for i in range(len(x1)):
  words1 += x1[i].split(' ')
c1 = Counter(words1)
print(c1)

"""Does the sum of the unique words in target 0 and 1 sum upto the total number of unique words in the whole document? Why or why not? Explain in report.

Total frequency
"""

unique_words = list(set(words))
unique_words0 = list(set(words0))
unique_words1 = list(set(words1))
print('Sum of unique words in class1 = ', len(unique_words1))
print('Sum of unique words in class0 = ', len(unique_words0))
print('Sum of unique words in both classes = ', len(unique_words))

"""Calculate the probability for each word in a given class.

Class 0
"""

probs0 = []
probs1 = []
for word in unique_words:
  if word in unique_words0:
    probs0.append(c0[word]/c[word])
  else:
    probs0.append(0)
  if word in unique_words1:
    probs1.append(c1[word]/c[word])
  else:
    probs1.append(0)

print(probs0)

"""Class 1"""

print(probs1)

"""We have calculated the probability of occurrence of word in a class, we can now substitute the values in the Baye's equation.

If a word from the new sentence does not occur in the class within the training set, the equation becomes zero. This problem can be solved using smoothing like Laplace smoothing.
"""

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
count_vect = CountVectorizer()
X_train_counts = count_vect.fit_transform(x_train)
tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)
X_train_tfidf = tf_transformer.transform(X_train_counts)
df_train = pd.DataFrame(X_train_tfidf.toarray(), columns=count_vect.get_feature_names())
df_train.head()

from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import cross_val_score
clf = MultinomialNB().fit(X_train_tfidf, y_train)
scores = cross_val_score(clf,X_train_tfidf,y_train,cv=5)
scores

from sklearn.pipeline import Pipeline
text_clf = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),('clf', MultinomialNB())])
text_clf.fit(x_train,y_train)
text_clf.score(x_test,y_test)

from sklearn import metrics
predicted = text_clf.predict(x_test)
print(metrics.classification_report(y_test, predicted, target_names=['class - 0','class - 1']))

metrics.confusion_matrix(y_test,predicted)

"""Probability for class 0"""

y_pred = text_clf.predict_proba(x_test)
y_pred[:,0]

"""Probability for class 1"""

y_pred[:,1]

"""Print target class"""

print(predicted)

print(metrics.classification_report(y_test, predicted, target_names=['class - 0','class - 1']))

"""References:

[Ref1](https://sebastianraschka.com/Articles/2014_naive_bayes_1.html)
"""